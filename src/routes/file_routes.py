"""File management routes for the SeekrAI platform.

This module provides Flask blueprint routes for managing files within the SeekrAI
application, including uploaded resume files, generated job result files, and
cache management operations. The routes support file downloads, cleanup operations,
and cache information display.

The module integrates with the ResumeProcessor for cache management and provides
a comprehensive file management interface for users to monitor and control their
uploaded files and generated results.

Key Features:
    - Secure file downloads with path traversal protection
    - File management dashboard showing upload and result statistics
    - Automated file cleanup based on age and type
    - Cache information display and management
    - Integration with ResumeProcessor cache system
    - File size and modification time tracking

Routes:
    - /download/<filename>: Download generated CSV files
    - /files: File management dashboard
    - /cleanup_files: AJAX endpoint for file cleanup operations
    - /cache: Cache information display
    - /clear_cache: Cache clearing operations

Security Features:
    - Path traversal protection using secure_filename()
    - File existence verification before serving
    - Comprehensive error handling and logging
    - CSRF protection for file operations

Dependencies:
    - Flask framework for web routing
    - Werkzeug for secure filename handling
    - ResumeProcessor for cache management
    - Pathlib for modern path operations

Example:
    >>> from flask import Flask
    >>> from routes.file_routes import file_bp
    >>> 
    >>> app = Flask(__name__)
    >>> app.register_blueprint(file_bp)
    >>> # Routes are now available at /download/, /files, etc.
"""

import os
from flask import Blueprint, render_template, request, jsonify, send_file, current_app, redirect, url_for, flash
from werkzeug.utils import secure_filename
from pathlib import Path
from resume_processor import ResumeProcessor
import logging

# Create blueprint
file_bp = Blueprint('files', __name__)

@file_bp.route('/download/<filename>')
def download_file(filename):
    """Download generated CSV file from job results folder.
    
    This endpoint provides secure file downloads for CSV files generated by the
    SeekrAI job analysis process. It implements security measures to prevent
    path traversal attacks and ensures only authorized files can be downloaded.
    
    The function serves files from the configured JOB_RESULTS_FOLDER directory,
    which typically contains CSV files with analyzed job data, keyword extractions,
    and search term results.
    
    Security Features:
        - Uses secure_filename() to prevent path traversal attacks
        - Verifies file existence before serving
        - Only serves files from the designated results folder
        - Comprehensive error handling and logging
    
    Args:
        filename (str): The name of the file to download. This should be just
            the filename without any path components. The function will
            automatically look for the file in the configured job results folder.
            
    Returns:
        Response: Flask response object containing the file for download with
            proper headers set for attachment download, or an error response:
            - 200: Successful file download with attachment headers
            - 404: File not found in the results folder
            - 500: Server error during file serving
    
    Raises:
        FileNotFoundError: If the requested file doesn't exist (returns 404)
        PermissionError: If file cannot be read due to permissions (returns 500)
        OSError: If there are file system errors (returns 500)
    
    Example:
        >>> # Download a generated job analysis CSV
        >>> response = client.get('/download/job_analysis_2024-01-15.csv')
        >>> assert response.status_code == 200
        >>> assert 'attachment' in response.headers['Content-Disposition']
        
        >>> # Attempt to download non-existent file
        >>> response = client.get('/download/nonexistent.csv')
        >>> assert response.status_code == 404
    
    Note:
        - Only files in the JOB_RESULTS_FOLDER can be downloaded
        - Filename is sanitized to prevent directory traversal
        - All download attempts are logged for security auditing
        - Files are served with 'attachment' disposition to force download
        - The endpoint is typically used after job analysis completion
    """
    logging.info(f"File download requested: {filename}")
    try:
        # Security check: ensure filename doesn't have path traversal
        safe_filename = secure_filename(filename)
        file_path = os.path.join(current_app.config['JOB_RESULTS_FOLDER'], safe_filename)
        
        if os.path.exists(file_path):
            logging.info(f"Serving file: {file_path}")
            return send_file(file_path, as_attachment=True)
        else:
            logging.warning(f"File not found: {file_path}")
            return "File not found", 404
            
    except Exception as e:
        logging.error(f"Error serving file {filename}: {str(e)}")
        return "Error downloading file", 500

@file_bp.route('/files')
def file_management():
    """Display the file management dashboard showing uploaded and generated files.
    
    This route provides a comprehensive file management interface that displays
    information about uploaded resume files and generated job result files. It
    shows file statistics including size, modification dates, and allows users
    to monitor their file usage within the SeekrAI platform.
    
    The dashboard provides two main categories of files:
    1. Uploaded Files: Resume files uploaded by users for analysis
    2. Result Files: CSV files generated from job analysis operations
    
    Features:
        - File size display in KB for easy reading
        - Modification time tracking for file management
        - Sorted display (newest files first)
        - File count statistics
        - Error handling for missing directories
        - Responsive web interface
    
    File Information Displayed:
        - File name and extension
        - File size in kilobytes (rounded to 2 decimal places)
        - Last modification timestamp
        - File count summaries
    
    Returns:
        Response: Rendered HTML template (files.html) with file information:
            - uploaded_files (List[Dict]): List of uploaded file information
            - result_files (List[Dict]): List of generated result files
            - error (str, optional): Error message if directory access fails
    
    Template Variables:
        uploaded_files: List of dictionaries containing:
            - name (str): Original filename
            - size (float): File size in KB
            - modified (float): Unix timestamp of last modification
            
        result_files: List of dictionaries containing:
            - name (str): Generated filename (typically CSV)
            - size (float): File size in KB  
            - modified (float): Unix timestamp of last modification
    
    Raises:
        OSError: If directory access fails (handled gracefully)
        PermissionError: If file stat operations fail (handled gracefully)
    
    Example:
        >>> # Access file management dashboard
        >>> response = client.get('/files')
        >>> assert response.status_code == 200
        >>> assert b'File Management' in response.data
        
        >>> # Check template context
        >>> with app.test_request_context('/files'):
        ...     response = file_management()
        ...     # Response contains file information for template
    
    Note:
        - Files are sorted by modification time (newest first)
        - Only files (not directories) are included in listings
        - File sizes are calculated from filesystem stat information
        - Missing directories are handled gracefully with empty lists
        - Error conditions are passed to template for user notification
    """
    logging.info("File management page requested")
    
    try:
        # Get uploaded files
        upload_folder = Path(current_app.config['UPLOAD_FOLDER'])
        uploaded_files = []
        if upload_folder.exists():
            for file_path in upload_folder.glob('*'):
                if file_path.is_file():
                    stat = file_path.stat()
                    uploaded_files.append({
                        'name': file_path.name,
                        'size': round(stat.st_size / 1024, 2),  # KB
                        'modified': stat.st_mtime
                    })
        
        # Get job result files
        results_folder = Path(current_app.config['JOB_RESULTS_FOLDER'])
        result_files = []
        if results_folder.exists():
            for file_path in results_folder.glob('*.csv'):
                if file_path.is_file():
                    stat = file_path.stat()
                    result_files.append({
                        'name': file_path.name,
                        'size': round(stat.st_size / 1024, 2),  # KB
                        'modified': stat.st_mtime
                    })
        
        # Sort by modification time (newest first)
        uploaded_files.sort(key=lambda x: x['modified'], reverse=True)
        result_files.sort(key=lambda x: x['modified'], reverse=True)
        
        logging.info(f"Found {len(uploaded_files)} uploaded files and {len(result_files)} result files")
        
        return render_template('files.html', 
                             uploaded_files=uploaded_files, 
                             result_files=result_files)
        
    except Exception as e:
        logging.error(f"Error in file management: {str(e)}", exc_info=True)
        return render_template('files.html', 
                             uploaded_files=[], 
                             result_files=[],
                             error="Error loading file information")

@file_bp.route('/cleanup_files', methods=['POST'])
def cleanup_files():
    """Clean up old files based on user-specified criteria via AJAX request.
    
    This endpoint provides automated file cleanup functionality for managing
    storage space and removing outdated files from the SeekrAI platform. It
    accepts JSON requests specifying cleanup parameters and returns detailed
    information about the cleanup operation.
    
    The cleanup operation can target different types of files:
    - 'uploads': Only uploaded resume files
    - 'results': Only generated job result files  
    - 'all': Both uploaded and result files
    
    Cleanup Criteria:
        - Age-based deletion using configurable day thresholds
        - File type filtering (uploads vs results vs all)
        - Size tracking for storage space recovery
        - Detailed logging of all cleanup operations
    
    Request Format:
        POST /cleanup_files
        Content-Type: application/json
        
        {
            "type": "uploads|results|all",
            "max_age_days": 7
        }
    
    Args:
        Request JSON body containing:
            type (str, optional): Type of files to clean up. Options:
                - 'uploads': Clean uploaded resume files only
                - 'results': Clean generated result files only  
                - 'all': Clean both uploads and results (default)
            max_age_days (int, optional): Maximum age in days for files to keep.
                Files older than this will be deleted. Defaults to 7 days.
    
    Returns:
        JSON Response: Cleanup operation results containing:
            On Success (200):
                - success (bool): True indicating successful operation
                - deleted_files (List[Dict]): Details of each deleted file:
                    - name (str): Filename that was deleted
                    - type (str): File type ('upload' or 'result')
                    - size (float): File size in KB
                - total_files_deleted (int): Count of files removed
                - total_size_freed_kb (float): Total storage space freed in KB
                
            On Error (500):
                - success (bool): False indicating operation failure
                - error (str): Error message describing the failure
    
    Raises:
        ValueError: If max_age_days cannot be converted to integer
        OSError: If file deletion operations fail
        PermissionError: If files cannot be deleted due to permissions
        JSONDecodeError: If request JSON is malformed
    
    Example:
        >>> # Clean up all files older than 30 days
        >>> response = client.post('/cleanup_files', 
        ...     json={'type': 'all', 'max_age_days': 30})
        >>> data = response.get_json()
        >>> print(f"Deleted {data['total_files_deleted']} files")
        >>> print(f"Freed {data['total_size_freed_kb']} KB")
        
        >>> # Clean only old result files
        >>> response = client.post('/cleanup_files',
        ...     json={'type': 'results', 'max_age_days': 14})
        >>> assert response.status_code == 200
    
    Security Considerations:
        - Only operates on files in configured upload/result directories
        - Uses secure file operations with proper error handling
        - Logs all deletion operations for audit trails
        - Validates input parameters to prevent malicious requests
    
    Note:
        - Cleanup operations are irreversible - deleted files cannot be recovered
        - The operation preserves directory structure, only removing files
        - Files currently being processed may be protected by the system
        - Cleanup statistics are logged for monitoring storage management
        - Failed individual file deletions are logged but don't stop the operation
    """
    logging.info("File cleanup request received")
    
    try:
        data = request.get_json()
        cleanup_type = data.get('type', 'all')  # 'uploads', 'results', or 'all'
        max_age_days = int(data.get('max_age_days', 7))
        
        from datetime import datetime, timedelta
        cutoff_time = datetime.now().timestamp() - (max_age_days * 24 * 60 * 60)
        
        deleted_files = []
        total_size_freed = 0
        
        def cleanup_directory(folder_path, file_type):
            """Helper function to clean up files in a specific directory.
            
            Args:
                folder_path (str): Path to the directory to clean up
                file_type (str): Type identifier for logging ('upload' or 'result')
            """
            nonlocal deleted_files, total_size_freed
            folder = Path(folder_path)
            if not folder.exists():
                return
                
            for file_path in folder.glob('*'):
                if file_path.is_file() and file_path.stat().st_mtime < cutoff_time:
                    try:
                        size = file_path.stat().st_size
                        file_path.unlink()
                        deleted_files.append({
                            'name': file_path.name,
                            'type': file_type,
                            'size': round(size / 1024, 2)
                        })
                        total_size_freed += size
                        logging.info(f"Deleted {file_type} file: {file_path.name}")
                    except Exception as e:
                        logging.error(f"Could not delete {file_path}: {e}")
        
        # Clean up based on type
        if cleanup_type in ['uploads', 'all']:
            cleanup_directory(current_app.config['UPLOAD_FOLDER'], 'upload')
        
        if cleanup_type in ['results', 'all']:
            cleanup_directory(current_app.config['JOB_RESULTS_FOLDER'], 'result')
        
        total_size_freed_kb = round(total_size_freed / 1024, 2)
        
        logging.info(f"Cleanup completed: {len(deleted_files)} files deleted, {total_size_freed_kb} KB freed")
        
        return jsonify({
            'success': True,
            'deleted_files': deleted_files,
            'total_files_deleted': len(deleted_files),
            'total_size_freed_kb': total_size_freed_kb
        })
        
    except Exception as e:
        logging.error(f"Error during file cleanup: {str(e)}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@file_bp.route('/cache')
def cache_info():
    """Display comprehensive cache information and statistics.
    
    This route provides a detailed view of the ResumeProcessor cache system,
    showing cache usage, file counts, storage consumption, and individual
    cache file information. It's designed for system administrators and users
    who want to monitor cache performance and storage usage.
    
    The cache system stores AI processing results to avoid redundant API calls
    to OpenAI, improving performance and reducing costs. This endpoint provides
    visibility into cache effectiveness and helps with cache management decisions.
    
    Cache Information Displayed:
        - Total number of cached response files
        - Total storage space used by cache files
        - Individual file details including size and age
        - Cache directory location and accessibility
        - Cache hit/miss statistics (if available)
    
    Returns:
        Response: Rendered HTML template (cache.html) displaying cache information:
            - cache_info (Dict): Comprehensive cache statistics and file details
            - error (str, optional): Error message if cache access fails
    
    Template Variables:
        cache_info: Dictionary containing cache statistics from ResumeProcessor:
            - cache_directory (str): Path to the cache directory
            - cache_files_count (int): Total number of cached files
            - total_size_mb (float): Total cache size in megabytes
            - files (List[Dict]): Individual cache file information:
                - name (str): Cache file name (usually hash-based)
                - size_bytes (int): File size in bytes
                - modified (float): Last modification timestamp
                - age_days (float): Age of the cache file in days
    
    Raises:
        CacheError: If cache directory is inaccessible (handled gracefully)
        OSError: If file system operations fail (handled gracefully)
        Exception: Any other cache-related errors (handled gracefully)
    
    Example:
        >>> # View cache information
        >>> response = client.get('/cache')
        >>> assert response.status_code == 200
        >>> assert b'Cache Information' in response.data
        
        >>> # Check cache statistics
        >>> with app.test_request_context('/cache'):
        ...     response = cache_info()
        ...     # Response contains cache statistics for template
    
    Integration:
        - Uses ResumeProcessor.get_cache_info() for data retrieval
        - Integrates with the application's cache management system
        - Provides data for cache monitoring and optimization decisions
        - Supports cache cleanup and maintenance operations
    
    Note:
        - Cache information is retrieved from the ResumeProcessor instance
        - Error conditions are handled gracefully with empty cache data
        - The interface provides insights for cache performance optimization
        - File information is sorted by modification time for easier analysis
        - Cache statistics help users understand AI processing cost savings
    """
    logging.info("Cache info requested")
    try:
        processor = ResumeProcessor()
        cache_data = processor.get_cache_info()
        return render_template('cache.html', cache_info=cache_data)
    except Exception as e:
        logging.error(f"Error getting cache info: {str(e)}")
        return render_template('cache.html', cache_info={}, error=str(e))

@file_bp.route('/clear_cache', methods=['POST'])
def clear_cache():
    """Clear the resume processing cache and redirect with status message.
    
    This endpoint provides cache clearing functionality for the ResumeProcessor
    cache system. It removes all cached AI responses, forcing fresh API calls
    for subsequent resume processing operations. This can be useful for debugging,
    testing, or when processing requirements change.
    
    The operation is irreversible and will result in increased API usage and
    processing time until the cache is rebuilt through normal processing operations.
    Users are notified of the operation results via flash messages.
    
    Cache Clearing Process:
        1. Initialize ResumeProcessor instance
        2. Call the clear_cache() method
        3. Process the results and calculate savings
        4. Display success/failure message to user
        5. Redirect back to cache information page
    
    Returns:
        Response: Redirect to the cache information page (/cache) with flash message:
            On Success: Shows number of files removed and space freed
            On Error: Shows error message describing the failure
    
    Flash Messages:
        Success: "Cache cleared successfully! X files removed, Y MB freed."
        Error: "Error clearing cache: [error_message]"
    
    Raises:
        CacheError: If cache clearing operation fails (handled gracefully)
        PermissionError: If cache files cannot be deleted (handled gracefully)
        OSError: If file system operations fail (handled gracefully)
        Exception: Any other cache-related errors (handled gracefully)
    
    Example:
        >>> # Clear cache via POST request
        >>> response = client.post('/clear_cache')
        >>> assert response.status_code == 302  # Redirect
        >>> assert response.location.endswith('/cache')
        
        >>> # Check for flash message in redirected page
        >>> response = client.get('/cache')
        >>> assert b'Cache cleared successfully' in response.data
    
    Integration:
        - Uses ResumeProcessor.clear_cache() for the actual operation
        - Integrates with Flask's flash messaging system
        - Redirects to cache information page for immediate feedback
        - Logs all cache clearing operations for audit purposes
    
    Security Considerations:
        - Requires POST method to prevent accidental cache clearing
        - Should be protected by authentication in production environments
        - Operation is logged for security auditing
        - No sensitive data is exposed in error messages
    
    Performance Impact:
        - Immediate: No performance impact on current operations
        - Subsequent: Increased processing time until cache rebuilds
        - API Usage: Higher OpenAI API usage until cache repopulates
        - Storage: Frees up cache storage space immediately
    
    Note:
        - Cache clearing is irreversible - cached data cannot be recovered
        - The operation affects all cached resume processing results
        - Fresh API calls will be made for all subsequent processing
        - Cache will be automatically rebuilt as new processing occurs
        - Users should consider the cost implications of clearing cache
    """
    logging.info("Cache clear request received")
    try:
        processor = ResumeProcessor()
        result = processor.clear_cache()
        flash(f'Cache cleared successfully! {result["files_removed"]} files removed, {result["space_freed_mb"]} MB freed.')
        logging.info("Cache cleared successfully")
    except Exception as e:
        logging.error(f"Error clearing cache: {str(e)}")
        flash(f'Error clearing cache: {str(e)}')
    
    return redirect(url_for('files.cache_info')) 